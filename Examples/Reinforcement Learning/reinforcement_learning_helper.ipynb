{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL Package-hvac.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vi60-9FuQo4"
      },
      "source": [
        "## DXC AI Starter: Reinforcement Learning\n",
        "\n",
        "<a target=\"_blank\" href=\"https://github.com/dxc-technology/DXC-Industrialized-AI-Starter\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baUgJ1rcvL6Z"
      },
      "source": [
        "## Set up the development environment\n",
        "\n",
        "This code installs all the packages you will need. Run it first. It should take 30 seconds or so to complete. If you get missing module errors later, it may be because you have not run this code. Restart the runtime/session after executing the below code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVTGOufCHvNy"
      },
      "source": [
        "! pip install git+https://github.com/dxc-technology/DXC-Industrialized-AI-Starter.git -qq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4_twg3QH3TA"
      },
      "source": [
        "from dxc import rl\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgml2E4SvyBJ"
      },
      "source": [
        "# Reinforcement Learning Basics\n",
        "\n",
        "Reinforcement learning is machine learning using **rewards** given to an **agent** acting in an **environment**. Instead of learning from historical data, the agent learns how to maneuver through the environment by receiving positive or negative rewards depending on the actions that it takes. \n",
        "\n",
        "\n",
        "### Train an OpenAI Gym environment\n",
        "\n",
        "Gym is a toolkit used for reinforcement learning created by OpenAI that includes several premade environments to test your models on. [View OpenAI Gym environments](https://gym.openai.com/envs/)\n",
        "\n",
        "### Models\n",
        "\n",
        "There are several models that can be used to train an agent in an environment. The model that needs to be used can be generally determined by the type of actions an agent can take. \n",
        "\n",
        "If a discrete set of actions are given, **DQN** or **SARSA** can be used. A discrete set of actions are actions that you can count. Examples of this include 3 actions for Rock, Paper, Scissors or 2 actions in TicTacToe for X and O. If the action space were between 1 to 5, the actions would be 1, 2, 3, 4, and 5.\n",
        "\n",
        "\n",
        "If a continous set of actions are given, **DDPG** can be used. A continuous set of actions are actions that you can measure. Examples of this include how fast a car should run (mph) or how likely a card will appear in a board game (%). If the action space were between 1 to 5, the actions would be all numbers between 1 to 5, including all decimal values.\n",
        "\n",
        "### Calling the Reinforcement Learning Helper\n",
        "\n",
        "`rl_helper()` accepts 2 parameters to run. The first parameter is the environment. The second parameter is the type of model the environment will train in. \n",
        "\n",
        "There are extra parameters that can be defined, but aren't necessary. These are used to further refine your model. Listing them down and defining the default values in parenthesis after the parameter name, the parameters that can be set for both discrete and continuous environments are:\n",
        "\n",
        "- steps (50000): number of episodes that the model will run for \n",
        "- saved_model_name (model): name of the saved model files\n",
        "- visualize (False): boolean (True or False) if there is visualization for the model, set True to display it\n",
        "\n",
        "The other parameters are only used for discrete environments.\n",
        "\n",
        "- test_steps (5): number of episodes the model will test\n",
        "- critic_hidden_layers (3): number of critic hidden layers\n",
        "- hidden_layers (3): number of hidden layers\n",
        "\n",
        "To call a gym environment, simply use the `gym.make()` function and pass the name of the gym environment. You can get the name of the gym environment [here](https://gym.openai.com/envs/). \n",
        "\n",
        "### Continuous Example\n",
        "\n",
        "In the following example code, the [Pendulum environment](https://gym.openai.com/envs/Pendulum-v0/) is used. This is an environment where the goal of the agent is to balance a pendulum upright. Unlike the cartpole example, the actions set for this environment are continuous, so DDPG will be used to train it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIUmbm-joCqP"
      },
      "source": [
        "env = gym.make(\"Pendulum-v0\")\n",
        "rl.rl_helper(env=env, \n",
        "             model_name=\"DDPG\", \n",
        "             saved_model_name=\"pendulum_model\", \n",
        "             steps=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWXknoDoG8A"
      },
      "source": [
        "\n",
        "### Discrete Example\n",
        "\n",
        "In the following example code, the [Cart Pole environment](https://gym.openai.com/envs/CartPole-v0/) is used. This is an environment where the goal of the agent (which is a cart) is to balance a pole. The model used is DQN (discrete actions) since the actions that the cart can take is move left or right. SARSA may also be used since it is a discrete environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FewRm-PTvxK_"
      },
      "source": [
        "import gym\n",
        "\n",
        "gym_env = gym.make(\"CartPole-v0\")\n",
        "rl.rl_helper(env=gym_env, \n",
        "             model_name=\"DQN\", #SARSA\n",
        "             steps=25000, \n",
        "             test_steps=5,\n",
        "             visualize=False,\n",
        "             saved_model_name=\"cartpole_model\",\n",
        "             critic_hidden_layers=2,\n",
        "             hidden_layers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue2tfhw4z-AJ"
      },
      "source": [
        "# Making your own environment\n",
        "\n",
        "Now that you've learned how to use the `rl_helper()`, you can explore how to create your own environment to train your own agent.\n",
        "\n",
        "Each environment has 3 main parts: \n",
        "\n",
        "`__init__()` is where the variables describing the environment is initialized. This includes variables that define what an agent can do and observe. This also includes the variables that define how the environment looks as the agent is going through it. It is important to define what the agent can observe since this is what the agent will base his choices in his action on. For example, in an environment where a car is running, if the road is slippery, the agent might want to choose a slower speed. \n",
        "\n",
        "In `__init__()`, it is important to define the following variables in the environment:\n",
        "\n",
        "*   `action_space` initializes the number of actions that are defined. \n",
        "*   `observation_space` initializes the number of variables that is observed by the agent\n",
        "*   `total_reward` initializes and later stores the rewards that the agent has\n",
        "*   `isDone` initializes and stores whether or not the agent is done going through the environment\n",
        "\n",
        "The rest of the variables defined in init are variables that just describe what the environment looks like to the agent.\n",
        "\n",
        "`reset()` is where the variables of the environment are reset when the environment is ran again. Generally, it will just look like `__init__()` since we are just setting the variables to its original values again. This is done because the agent will train in the environment multiple times. To make sure each run independent of each other, the variables need to be reset to their original values. \n",
        "\n",
        "It is important to note that `reset()` should return what the agent can observe.\n",
        "\n",
        "`step()` is where the steps that the agent takes through the environment is defined. \n",
        "\n",
        "## Making a **Rock, Paper, Scissors** environment\n",
        "\n",
        "The goal of the game is to choose an item that beats the other item. \n",
        "\n",
        "*   Rock beats scissors. Rock loses to paper.\n",
        "*   Paper beats rock. Paper loses to scissors.\n",
        "*   Scissors beats paper. Scissors loses to rock.\n",
        "\n",
        "In the following environment, each item is defined by an integer:\n",
        "\n",
        "*   0 - Rock\n",
        "*   1 - Paper\n",
        "*   2 - Scissors\n",
        "\n",
        "Since there are 3 discrete actions that the agent can take, `action_space` is set to `space.Discrete(3)` to define 3 discrete actions. The agent would only observe what the opponent chooses so `observation_space` is set to `space.Discrete(1)` to define 1 observable variable. The action of the opponent is one of the items chosen at random so `opponent_action` is defined as `np.random.randint(3)`.\n",
        "\n",
        "In `step()`, we define the steps that the agent takes. In this case, we pass the action that the agent takes. Using this action, we can compare it to the action that the opponent takes and determine the reward that the agent gets. This is defined in `determine_reward()`. It is a simple function that uses if else statements to compare the actions that the agent and opponent takes. If their action is the same, the reward that the agent gets is 0. If the agent beats the opponent, the agent gets 1. If the agent loses to the opponent, the agent gets 0. \n",
        "\n",
        "Once the reward for the current step is determined, it is added to the total rewards of the whole run. We then determine if the run is over by checking what round the agent is on. If `round_number` is not equal to the `max_round`, then `round_number` is iterated. Otherwise, isDone is set to True causing the run of the environment to be over. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-K-Q4qKXRW-"
      },
      "source": [
        "class basic_env(object):\n",
        "  def __init__(self):\n",
        "    self.action_space = spaces.Discrete(3)\n",
        "    self.observation_space = spaces.Discrete(1)\n",
        "    self.total_reward = 0\n",
        "    self.isDone = False\n",
        "\n",
        "    self.opponent_action = np.random.randint(3)\n",
        "    self.round_number = 1\n",
        "    self.max_rounds = 10\n",
        "    \n",
        "  def reset(self):\n",
        "    self.action_space = spaces.Discrete(3)\n",
        "    self.observation_space = spaces.Discrete(1)\n",
        "    self.total_reward = 0\n",
        "    self.isDone = False\n",
        "\n",
        "    self.opponent_action = np.random.randint(3)\n",
        "    self.round_number = 1\n",
        "    self.max_rounds = 10\n",
        "\n",
        "    return np.array(self.opponent_action)\n",
        "\n",
        "  def step(self, action):\n",
        "    reward = basic_env.determine_reward(self, action, self.opponent_action)\n",
        "    self.total_reward = self.total_reward + reward\n",
        "\n",
        "    if self.round_number != self.max_rounds:\n",
        "      self.round_number += 1\n",
        "    else:\n",
        "      self.isDone = True\n",
        "      \n",
        "    basic_env.determine_opponent_action(self)\n",
        "    return np.array(self.opponent_action), reward, self.isDone, {}\n",
        "\n",
        "  ####################\n",
        "  ##helper functions##\n",
        "  ####################\n",
        "  def determine_opponent_action(self):\n",
        "    self.opponent_action = np.random.randint(3)\n",
        "\n",
        "  def determine_reward(self, agent_choice, opponent_choice):\n",
        "    reward = 0\n",
        "    if agent_choice == opponent_choice: #the same choice\n",
        "      reward = 0\n",
        "    elif agent_choice == 0: #rock\n",
        "      if opponent_choice == 1: #against paper\n",
        "        reward = 0\n",
        "      else: #against scissors\n",
        "        reward = 1\n",
        "    elif agent_choice == 1: #paper\n",
        "      if opponent_choice == 2: #against scissors\n",
        "        reward = 0\n",
        "      else: #against rock\n",
        "        reward = 1\n",
        "    elif agent_choice == 2: #scissors\n",
        "      if opponent_choice == 0: #against rock\n",
        "        reward = 0\n",
        "      else: #against paper\n",
        "        reward = 1\n",
        "\n",
        "    return reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrJ9NoCQYUBX"
      },
      "source": [
        "## Calling your custom environment\n",
        "\n",
        "Simply pass your environment to the same `rl_helper()` function.\n",
        "\n",
        "`rl_helper()` is set to train the model in 50,000 episodes. Once this training is done, the saved model will be tested 5 times. In the Rock, Paper, Scissors environment, the ideal total reward that the agent can get is 10. The agent can get a total reward of 10 if it wins each of the 10 rounds of Rock, Paper, Scissors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnyRdCt4Idcp"
      },
      "source": [
        "env = basic_env()\n",
        "rl.rl_helper(env, \"DQN\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mivrQ85UuyL1"
      },
      "source": [
        "## HVAC env\n",
        "\n",
        "To save on energy, several companies such as Google use Reinforcement Learning to curb their energy use. The following is an example of a simplified HVAC environment.\n",
        "\n",
        "The goal of the agent in this example is to keep the environment temperature as close to the desired room temperature for as long as possible. \n",
        "\n",
        "To do this, we establish the desired temperature, the time intervals, the external temperature that affects the internal temperature, how fast the internal temperature assumes the external temperature, and how fast the HVAC temperature affects the internal temperature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-SzR34Cu0ww"
      },
      "source": [
        "import random\n",
        "class hvac_env(object):\n",
        "\tdef __init__(self):\n",
        "\t\tself.action_space = spaces.Discrete(3)\n",
        "\t\tself.observation_space = spaces.Discrete(1)\n",
        "\t\tself.isDone = False\n",
        "\n",
        "\t\tself.current_hour = 0\n",
        "\t\tself.hour = 24\n",
        "\n",
        "\t\tself.target = 60\n",
        "\t\tself.target_condition_range = 5 # acceptable range deviation from target\n",
        "\n",
        "\t\tself.temp_increment = 5\n",
        "\n",
        "\t\tself.target_reward = 1\n",
        "\t\tself.out_of_target_reward = -2\n",
        "\t\tself.efficiency_reward = 1\n",
        "\n",
        "\t\tself.range = random.choice([(40, 50), (70, 80)])\n",
        "\t\tself.current_condition = np.random.randint(self.range[0],self.range[1])\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tself.temp_increment = 5\n",
        "\t\tself.isDone = False\n",
        "\t\tself.range = random.choice([(40, 50), (70, 80)])\n",
        "\t\tself.current_condition = np.random.randint(self.range[0],self.range[1])\n",
        "\t\tself.current_hour = 0\n",
        "\n",
        "\t\treturn np.array(self.current_condition)\n",
        "\n",
        "\tdef step(self, action):\n",
        "\t\tepisode_reward = 0\n",
        "\n",
        "\t\tif action == 0: #heat\n",
        "\t\t\tself.current_condition += self.temp_increment \n",
        "\t\telif action == 1: #cool\n",
        "\t\t\tself.current_condition -= self.temp_increment \n",
        "\t\telif action == 2: #do nothing\n",
        "\t\t\tself.current_condition = self.current_condition\n",
        "\t\t\tepisode_reward += self.efficiency_reward\n",
        "\n",
        "\t\tif self.current_condition >= self.target - self.target_condition_range and self.current_condition <= self.target + self.target_condition_range:\n",
        "\t\t\tepisode_reward += self.target_reward #reward for being within the target range\n",
        "\t\telse:\n",
        "\t\t\tepisode_reward += self.out_of_target_reward #penalty for not being within the range\n",
        "\n",
        "\t\tserver_heat = np.random.randint(0, 3)\n",
        "\t\t#print(\"Episode reward: {} | Total reward: {}\".format(episode_reward, self.total_reward))\n",
        "\t\t#print(\"Action: {} | Current Condition: {} | Heated Condition: {} | Episode Reward: {}\".format(action, self.current_condition, (self.current_condition+server_heat), episode_reward))\n",
        "\t\tself.current_condition += server_heat\n",
        "\n",
        "\t\tif self.current_hour != self.hour:\n",
        "\t\t\tself.current_hour += 1\n",
        "\t\telse:\n",
        "\t\t\tself.isDone = True\n",
        "\n",
        "\t\treturn np.array(self.current_condition), episode_reward, self.isDone, {}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqF9ynifu3uq"
      },
      "source": [
        "hvac_env = hvac_env()\n",
        "\n",
        "rl.rl_helper(env=gym_env, \n",
        "             model_name=\"DQN\", \n",
        "             steps=50000, \n",
        "             test_steps=5,\n",
        "             visualize=False,\n",
        "             saved_model_name=\"hvac_model\",\n",
        "             critic_hidden_layers=2,\n",
        "             hidden_layers=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}